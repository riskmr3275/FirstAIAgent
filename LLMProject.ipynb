{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"facebook/opt-350m\"  # Change to the larger model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Set pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load and chunk the data\n",
    "def load_and_chunk_book(file_path, chunk_size=8):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    random.seed(42)\n",
    "    random.shuffle(lines)\n",
    "    chunks = [\" \".join(lines[i:i+chunk_size]) for i in range(0, len(lines), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "file_path = \"/content/sample_data/Krishna_Leelas.txt\"  # Update if needed\n",
    "text_chunks = load_and_chunk_book(file_path)\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(text, max_length=256):\n",
    "    return tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "# Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.examples = []\n",
    "        for text in texts:\n",
    "            tokenized = tokenize_function(text)\n",
    "            input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "            attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n",
    "            labels = input_ids.clone()\n",
    "            labels[labels == tokenizer.pad_token_id] = -100\n",
    "            self.examples.append({\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"labels\": labels\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val.to(device) for key, val in self.examples[idx].items()}\n",
    "\n",
    "dataset = TextDataset(text_chunks)\n",
    "\n",
    "# Train-test split\n",
    "train_size = int(0.9 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)  # Reduce batch size for large model\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=2)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training\n",
    "epochs = 5\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(loop):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Average Training Loss: {avg_loss:.4f}\")\n",
    "    print(\"üß† Sample Output After Epoch:\")\n",
    "    print(\"‚û§\", tokenizer.decode(model.generate(tokenizer(\"Lord Krishna was known for\", return_tensors=\"pt\").to(device)[\"input_ids\"], max_length=60, do_sample=True, top_k=50, top_p=0.95, temperature=0.8)[0], skip_special_tokens=True))\n",
    "\n",
    "# Save model\n",
    "save_dir = \"/content/opt350m_b_finetuned\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"‚úÖ Model saved to {save_dir}\")\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(prompt, max_length=60):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=256).to(device)  # Set max_length for tokenization\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length + len(inputs[\"input_ids\"][0]),  # Total length including the input tokens\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=1.0,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the generation again\n",
    "prompt = \"How did Krishna defeat the demon?\"\n",
    "print(\"üìù Generated Text:\")\n",
    "print(generate_text(prompt))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
